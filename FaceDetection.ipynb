{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d507ee8-74ec-414b-b69a-df2d2ced2b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install labelme tensorflow opencv-python matplotlib albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31308b4e-004f-4dbd-bb07-cbc59ae2e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "995f51c7-ae02-4667-9cc0-c786d6b319c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = os.path.join('data','images')\n",
    "number_images = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf4fa49-4e0d-49a5-9bc5-de4c770c9799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "for imgnum in range(number_images):\n",
    "    print('Collecting image {}'.format(imgnum))\n",
    "    ret, frame = cap.read()\n",
    "    imgname = os.path.join(IMAGES_PATH,f'{str(uuid.uuid1())}.jpg')\n",
    "    cv2.imwrite(imgname, frame)\n",
    "    cv2.imshow('frame', frame)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8427d52-c888-4d46-b811-319f821fd0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!labelme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e35321d-bb6f-4336-a4a3-a86d6ff6b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ff26e29-bd33-4238-96da-7dfa647f96c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0165592d-c679-4726-8bf5-cff49d9fcdb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d3346dc-da18-40ee-879e-166ee17f4b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TF GPUs:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3af381cc-8cfe-4458-beda-521d623df182",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = tf.data.Dataset.list_files('data\\\\images\\\\*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7450fd5c-b524-4957-a977-7b5a616855e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'data\\\\images\\\\da651f4a-66b2-11f0-9d8d-e08f4ce6c534.jpg'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69b37b69-92a9-4728-aec9-6c47b502472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(x): \n",
    "    byte_img = tf.io.read_file(x)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e25f0992-f6e8-4911-9110-6c6c059f9bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.map(load_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46c54811-7826-4541-8b5d-0f88dde4f80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[135, 128, 112],\n",
       "        [135, 128, 112],\n",
       "        [135, 128, 112],\n",
       "        ...,\n",
       "        [158, 152, 136],\n",
       "        [157, 151, 135],\n",
       "        [157, 151, 135]],\n",
       "\n",
       "       [[134, 127, 111],\n",
       "        [134, 127, 111],\n",
       "        [134, 127, 111],\n",
       "        ...,\n",
       "        [159, 153, 137],\n",
       "        [156, 150, 134],\n",
       "        [156, 150, 134]],\n",
       "\n",
       "       [[134, 127, 111],\n",
       "        [134, 127, 111],\n",
       "        [134, 126, 113],\n",
       "        ...,\n",
       "        [159, 153, 137],\n",
       "        [157, 151, 135],\n",
       "        [156, 150, 134]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[150, 152, 147],\n",
       "        [150, 152, 147],\n",
       "        [151, 153, 148],\n",
       "        ...,\n",
       "        [139, 133, 135],\n",
       "        [138, 132, 134],\n",
       "        [137, 131, 133]],\n",
       "\n",
       "       [[149, 151, 146],\n",
       "        [148, 150, 145],\n",
       "        [150, 152, 147],\n",
       "        ...,\n",
       "        [138, 133, 137],\n",
       "        [137, 132, 136],\n",
       "        [137, 132, 136]],\n",
       "\n",
       "       [[148, 150, 145],\n",
       "        [147, 149, 144],\n",
       "        [150, 152, 147],\n",
       "        ...,\n",
       "        [137, 132, 136],\n",
       "        [136, 131, 135],\n",
       "        [136, 131, 135]]], dtype=uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d5b58af-7dee-427a-85a7-7540584b1039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.MapDataset"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29543e37-a028-44cd-92f4-46cb536cc871",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_generator = images.batch(4).as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee1a2c21-caa3-49b8-b0d7-602b6408d622",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images = image_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e3e647-81b2-48ec-b09f-6aa2968d7a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx, image in enumerate(plot_images):\n",
    "    ax[idx].imshow(image) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e9aaa91-0d85-49b1-92a8-0f7e79ffeb67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.99999999999999"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "90*.7 # 63 to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f24313e3-cf82-486e-a3bf-e737b5a704d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "90*.15 # 14 and 13 to test and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6972d4a3-8755-44e0-99c2-143c1348a422",
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in ['train','test','val']:\n",
    "    for file in os.listdir(os.path.join('data', folder, 'images')):\n",
    "        \n",
    "        filename = file.split('.')[0]+'.json'\n",
    "        existing_filepath = os.path.join('data','labels', filename)\n",
    "        if os.path.exists(existing_filepath): \n",
    "            new_filepath = os.path.join('data',folder,'labels',filename)\n",
    "            os.replace(existing_filepath, new_filepath)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb5e431f-5f2e-4e27-9187-1ac613a2cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(os.path.join('data','train','images','00bf06b7-66b3-11f0-8269-e08f4ce6c534.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "741867bd-c350-45cd-bb2d-a75b2b7d7bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 640, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b0fd0a9-1c78-4c89-9cca-e0b6f152305a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albumentations version: 2.0.8\n"
     ]
    }
   ],
   "source": [
    "import albumentations as alb\n",
    "print(\"Albumentations version:\", alb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8be9bb97-4440-44c5-99b5-f4d8878631ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentor = alb.Compose([alb.RandomCrop(width=450, height=450), \n",
    "                         alb.HorizontalFlip(p=0.5), \n",
    "                         alb.RandomBrightnessContrast(p=0.2),\n",
    "                         alb.RandomGamma(p=0.2), \n",
    "                         alb.RGBShift(p=0.2), \n",
    "                         alb.VerticalFlip(p=0.5)], \n",
    "                       bbox_params=alb.BboxParams(format='albumentations', \n",
    "                                                  label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a23b58f4-816e-471a-a819-49005838c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(os.path.join('data','train', 'images','04af140d-66b3-11f0-a5a1-e08f4ce6c534.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99e9cb70-94de-4c7e-a732-7ef04e6c348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data', 'train', 'labels', '04af140d-66b3-11f0-a5a1-e08f4ce6c534.json'), 'r') as f:\n",
    "    label = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3aacc00-7e25-4802-9dde-a8f2a77345e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[198.85304659498206, 107.20430107526882],\n",
       " [441.1469534050179, 451.2903225806452]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label['shapes'][0]['points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a86b83c9-273c-4f89-a8be-216ffb269933",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = [0,0,0,0]\n",
    "coords[0] = label['shapes'][0]['points'][0][0]\n",
    "coords[1] = label['shapes'][0]['points'][0][1]\n",
    "coords[2] = label['shapes'][0]['points'][1][0]\n",
    "coords[3] = label['shapes'][0]['points'][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0f1690e-d7e0-443b-b9d8-5298c9b7178b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[198.85304659498206, 107.20430107526882, 441.1469534050179, 451.2903225806452]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09f52b32-3ca0-45f1-b63e-770e759877cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = list(np.divide(coords, [640,480,640,480]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3cf2bb6-f0c7-4bf5-884c-0c46dda2ce8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3107078853046595,\n",
       " 0.22334229390681004,\n",
       " 0.6892921146953405,\n",
       " 0.9401881720430108]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf3e1066-d248-4fd9-90a3-40e28115b6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented = augmentor(image=img, bboxes=[coords], class_labels=['face'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e393d8ba-842f-4b47-91e8-4cd8e733b454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7625488111707899, 0.8017682139078778]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented['bboxes'][0][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ece719fd-77e7-4602-b2d5-a8d73866ad8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.2241178978814019,\n",
       "  0.03713261922200517,\n",
       "  0.7625488111707899,\n",
       "  0.8017682139078778]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented['bboxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424c0b4e-6031-4274-8202-1a54ad2f1e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.rectangle(augmented['image'], \n",
    "              tuple(np.multiply(augmented['bboxes'][0][:2], [450,450]).astype(int)),\n",
    "              tuple(np.multiply(augmented['bboxes'][0][2:], [450,450]).astype(int)), \n",
    "                    (255,0,0), 2)\n",
    "\n",
    "plt.imshow(augmented['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "000ac11a-b574-438b-ae91-5b902cfcded8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_max is less than or equal to x_min for bbox [0.36279121 0.85580945 0.         0.11652279 0.        ].\n"
     ]
    }
   ],
   "source": [
    "for partition in ['train','test','val']: \n",
    "    for image in os.listdir(os.path.join('data', partition, 'images')):\n",
    "        img = cv2.imread(os.path.join('data', partition, 'images', image))\n",
    "\n",
    "        coords = [0,0,0.00001,0.00001]\n",
    "        label_path = os.path.join('data', partition, 'labels', f'{image.split(\".\")[0]}.json')\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                label = json.load(f)\n",
    "\n",
    "            coords[0] = label['shapes'][0]['points'][0][0]\n",
    "            coords[1] = label['shapes'][0]['points'][0][1]\n",
    "            coords[2] = label['shapes'][0]['points'][1][0]\n",
    "            coords[3] = label['shapes'][0]['points'][1][1]\n",
    "            coords = list(np.divide(coords, [640,480,640,480]))\n",
    "\n",
    "        try: \n",
    "            for x in range(60):\n",
    "                augmented = augmentor(image=img, bboxes=[coords], class_labels=['face'])\n",
    "                cv2.imwrite(os.path.join('aug_data', partition, 'images', f'{image.split(\".\")[0]}.{x}.jpg'), augmented['image'])\n",
    "\n",
    "                annotation = {}\n",
    "                annotation['image'] = image\n",
    "\n",
    "                if os.path.exists(label_path):\n",
    "                    if len(augmented['bboxes']) == 0: \n",
    "                        annotation['bbox'] = [0,0,0,0]\n",
    "                        annotation['class'] = 0 \n",
    "                    else: \n",
    "                        annotation['bbox'] = augmented['bboxes'][0]\n",
    "                        annotation['class'] = 1\n",
    "                else: \n",
    "                    annotation['bbox'] = [0,0,0,0]\n",
    "                    annotation['class'] = 0 \n",
    "\n",
    "\n",
    "                with open(os.path.join('aug_data', partition, 'labels', f'{image.split(\".\")[0]}.{x}.json'), 'w') as f:\n",
    "                    json.dump(annotation, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49ad61a4-8a42-4ff9-a9b0-3ff8f73939aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = tf.data.Dataset.list_files('aug_data\\\\train\\\\images\\\\*.jpg', shuffle=False)\n",
    "train_images = train_images.map(load_image)\n",
    "train_images = train_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "train_images = train_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a753ba5-2fe5-4fd2-89b2-0e69556e6ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = tf.data.Dataset.list_files('aug_data\\\\test\\\\images\\\\*.jpg', shuffle=False)\n",
    "test_images = test_images.map(load_image)\n",
    "test_images = test_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "test_images = test_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ce7f75a-cac0-4425-b350-953d2aa9d5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = tf.data.Dataset.list_files('aug_data\\\\val\\\\images\\\\*.jpg', shuffle=False)\n",
    "val_images = val_images.map(load_image)\n",
    "val_images = val_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "val_images = val_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d67efc60-b0b9-4942-b33a-643357842728",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "514da4fc-862b-4e14-a165-72fbd2c6211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(label_path):\n",
    "    with open(label_path.numpy(), 'r', encoding = \"utf-8\") as f:\n",
    "        label = json.load(f)\n",
    "        \n",
    "    return [label['class']], label['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40b360e5-3dd4-48ba-928a-d2e0dd7899df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tf.data.Dataset.list_files('aug_data\\\\train\\\\labels\\\\*.json', shuffle=False)\n",
    "train_labels = train_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "356fff2c-a00c-4daf-a33d-f73283191b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = tf.data.Dataset.list_files('aug_data\\\\test\\\\labels\\\\*.json', shuffle=False)\n",
    "test_labels = test_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60e55a91-6a96-4cb0-9597-b0b3c637e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = tf.data.Dataset.list_files('aug_data\\\\val\\\\labels\\\\*.json', shuffle=False)\n",
    "val_labels = val_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ec80685-2c8f-40dc-986a-bdd334d5297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e96e2d37-2c70-4be0-99d2-afdecfa14fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_images), len(train_labels), len(test_images), len(test_labels), len(val_images), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f36dda1-fd7e-4676-83d1-598a15ed17ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.data.Dataset.zip((train_images, train_labels))\n",
    "train = train.shuffle(5000)\n",
    "train = train.batch(8)\n",
    "train = train.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e856194f-f730-45c6-ac65-96446ed420c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tf.data.Dataset.zip((test_images, test_labels))\n",
    "test = test.shuffle(1300)\n",
    "test = test.batch(8)\n",
    "test = test.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7f3d7e97-5121-4439-b89e-499954710c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = tf.data.Dataset.zip((val_images, val_labels))\n",
    "val = val.shuffle(1000)\n",
    "val = val.batch(8)\n",
    "val = val.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "275b5692-70cb-47e4-8f9d-a862f82d9c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.as_numpy_iterator().next()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "33061a2b-96a3-48ef-8904-4755022b329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = train.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fe8f331b-32da-4f76-8732-43628909c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = data_samples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eef868-bcb6-453d-a2fa-f614cce4b36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx in range(4): \n",
    "    sample_image = res[0][idx].copy()\n",
    "    sample_coords = res[1][1][idx]\n",
    "    \n",
    "    cv2.rectangle(sample_image, \n",
    "                  tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
    "                  tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)), \n",
    "                        (255,0,0), 2)\n",
    "\n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8734789b-f7a1-4583-bed5-c6aba2d1bda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, GlobalMaxPooling2D\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "840f8f6d-b902-49ee-8f6a-175466b5b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32349028-9bc6-44fc-958d-08a986ad6439",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "df20864d-a7b8-4739-bb68-9411b843e13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(): \n",
    "    input_layer = Input(shape=(120,120,3))\n",
    "    \n",
    "    vgg = VGG16(include_top=False)(input_layer)\n",
    "\n",
    "    # Classification Model  \n",
    "    f1 = GlobalMaxPooling2D()(vgg)\n",
    "    class1 = Dense(2048, activation='relu')(f1)\n",
    "    class2 = Dense(1, activation='sigmoid')(class1)\n",
    "    \n",
    "    # Bounding box model\n",
    "    f2 = GlobalMaxPooling2D()(vgg)\n",
    "    regress1 = Dense(2048, activation='relu')(f2)\n",
    "    regress2 = Dense(4, activation='sigmoid')(regress1)\n",
    "    \n",
    "    facetracker = Model(inputs=input_layer, outputs=[class2, regress2])\n",
    "    return facetracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cb983735-5617-43a5-be47-6329f1042f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ddfa0455-6329-47c8-ac66-165abf0863cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 120, 120, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " vgg16 (Functional)             (None, None, None,   14714688    ['input_2[0][0]']                \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " global_max_pooling2d (GlobalMa  (None, 512)         0           ['vgg16[0][0]']                  \n",
      " xPooling2D)                                                                                      \n",
      "                                                                                                  \n",
      " global_max_pooling2d_1 (Global  (None, 512)         0           ['vgg16[0][0]']                  \n",
      " MaxPooling2D)                                                                                    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 2048)         1050624     ['global_max_pooling2d[0][0]']   \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 2048)         1050624     ['global_max_pooling2d_1[0][0]'] \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            2049        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4)            8196        ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16,826,181\n",
      "Trainable params: 16,826,181\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "facetracker.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "835ce4da-e113-475c-a0a5-68fea2ada36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6fa24866-3820-48dd-af07-fd048a4b7b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 120, 120, 3)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8b37168f-b934-46da-aaf5-787a1420ae72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 9s 9s/step\n"
     ]
    }
   ],
   "source": [
    "classes, coords = facetracker.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "20512713-8c72-4353-9b77-7184ae6113a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.47895613],\n",
       "        [0.39612255],\n",
       "        [0.4812824 ],\n",
       "        [0.37061238],\n",
       "        [0.336393  ],\n",
       "        [0.41636106],\n",
       "        [0.36207217],\n",
       "        [0.433299  ]], dtype=float32),\n",
       " array([[0.5342557 , 0.5077865 , 0.36887684, 0.3768045 ],\n",
       "        [0.61427   , 0.4652597 , 0.36483997, 0.44269985],\n",
       "        [0.5822294 , 0.55461   , 0.36951026, 0.46787554],\n",
       "        [0.60734767, 0.5163387 , 0.4041003 , 0.42946318],\n",
       "        [0.5930519 , 0.49898878, 0.38484025, 0.4162153 ],\n",
       "        [0.57660633, 0.4627244 , 0.4209815 , 0.4016859 ],\n",
       "        [0.5835676 , 0.4494453 , 0.4302397 , 0.43993184],\n",
       "        [0.55748236, 0.51572424, 0.33508745, 0.3451993 ]], dtype=float32))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes, coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5a45f9c9-bacd-47a4-89f5-42ec6d0fe7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch = len(train)\n",
    "lr_decay = (1./0.75 -1)/batches_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9ceb95db-927f-42b8-ab31-dd00283fead7",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.0001, decay=lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "16cc4b58-6b79-45b3-a8ec-6f18262a8efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def localization_loss(y_true, yhat):            \n",
    "    delta_coord = tf.reduce_sum(tf.square(y_true[:,:2] - yhat[:,:2]))\n",
    "                  \n",
    "    h_true = y_true[:,3] - y_true[:,1] \n",
    "    w_true = y_true[:,2] - y_true[:,0] \n",
    "\n",
    "    h_pred = yhat[:,3] - yhat[:,1] \n",
    "    w_pred = yhat[:,2] - yhat[:,0] \n",
    "    \n",
    "    delta_size = tf.reduce_sum(tf.square(w_true - w_pred) + tf.square(h_true-h_pred))\n",
    "    \n",
    "    return delta_coord + delta_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1b0e8f05-7e70-420c-bb25-236c8d147fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "classloss = tf.keras.losses.BinaryCrossentropy()\n",
    "regressloss = localization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6aa3f3af-fd04-46ee-acab-7274e8aefe75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=10.732641>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "localization_loss(y[1], coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d7930e2a-3352-4ad4-9925-c020313bdbcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.8249371>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classloss(y[0], classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "06f02185-50d5-4b0d-949c-4c3a8bfa639f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=10.732641>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressloss(y[1], coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9d4e9321-4fcd-4e64-9e93-e0e7faed02a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceTracker(Model): \n",
    "    def __init__(self, eyetracker,  **kwargs): \n",
    "        super().__init__(**kwargs)\n",
    "        self.model = eyetracker\n",
    "\n",
    "    def compile(self, opt, classloss, localizationloss, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.closs = classloss\n",
    "        self.lloss = localizationloss\n",
    "        self.opt = opt\n",
    "    \n",
    "    def train_step(self, batch, **kwargs): \n",
    "        \n",
    "        X, y = batch\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            classes, coords = self.model(X, training=True)\n",
    "            \n",
    "            batch_classloss = self.closs(y[0], classes)\n",
    "            batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "            \n",
    "            total_loss = batch_localizationloss+0.5*batch_classloss\n",
    "            \n",
    "            grad = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "        \n",
    "        opt.apply_gradients(zip(grad, self.model.trainable_variables))\n",
    "        \n",
    "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n",
    "    \n",
    "    def test_step(self, batch, **kwargs): \n",
    "        X, y = batch\n",
    "        \n",
    "        classes, coords = self.model(X, training=False)\n",
    "        \n",
    "        batch_classloss = self.closs(y[0], classes)\n",
    "        batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "        total_loss = batch_localizationloss+0.5*batch_classloss\n",
    "        \n",
    "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n",
    "        \n",
    "    def call(self, X, **kwargs): \n",
    "        return self.model(X, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ba28ae1c-2f5e-4aef-944e-5317e3443493",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FaceTracker(facetracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "66a588e5-4c8b-4e9b-bcbf-79d182d253a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(opt, classloss, regressloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "550e0c3d-b733-46bd-bfdc-05e107f29866",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir='logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8cf6f101-cb93-4fcd-896e-a9f057932153",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d27f7ae-1d81-4ae5-a0fb-2f641ef7a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train, epochs=10, validation_data=val, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adcecaf9-1763-438a-a227-e59b48df5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b3c988a-4621-40e8-aed6-5a71bec9a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(20,5))\n",
    "\n",
    "ax[0].plot(hist.history['total_loss'], color='teal', label='loss')\n",
    "ax[0].plot(hist.history['val_total_loss'], color='orange', label='val loss')\n",
    "ax[0].title.set_text('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(hist.history['class_loss'], color='teal', label='class loss')\n",
    "ax[1].plot(hist.history['val_class_loss'], color='orange', label='val class loss')\n",
    "ax[1].title.set_text('Classification Loss')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(hist.history['regress_loss'], color='teal', label='regress loss')\n",
    "ax[2].plot(hist.history['val_regress_loss'], color='orange', label='val regress loss')\n",
    "ax[2].title.set_text('Regression Loss')\n",
    "ax[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "407e4d31-247b-4488-9f1e-8060eb760a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "031e35fc-363c-4394-b886-41123e301199",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test_data.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b89c8d3f-4fcb-43aa-b252-2e44cce5a9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n"
     ]
    }
   ],
   "source": [
    "yhat = facetracker.predict(test_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f5c58c-d4a0-419b-9d97-550c5310789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx in range(4): \n",
    "    sample_image = test_sample[0][idx].copy()\n",
    "    sample_coords = yhat[1][idx]\n",
    "    \n",
    "    if yhat[0][idx] > 0.9:\n",
    "        cv2.rectangle(sample_image, \n",
    "                      tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "    \n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "06301a20-ad2c-4fb4-9e9a-4a5b9dfd8cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec630267-9aef-4ade-97be-e3c508e64982",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker.save('facetracker.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b011a3fe-0d9f-47b0-9694-2cc160a0b05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker = load_model('facetracker.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20d0057a-d71b-4b69-bb14-84acb310af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(0)\n",
    "# while cap.isOpened():\n",
    "#     _ , frame = cap.read()\n",
    "#     frame = frame[50:500, 50:500,:]\n",
    "    \n",
    "#     rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#     resized = tf.image.resize(rgb, (120,120))\n",
    "    \n",
    "#     yhat = facetracker.predict(np.expand_dims(resized/255,0))\n",
    "#     sample_coords = yhat[1][0]\n",
    "    \n",
    "#     if yhat[0] > 0.5: \n",
    "#         # Controls the main rectangle\n",
    "#         cv2.rectangle(frame, \n",
    "#                       tuple(np.multiply(sample_coords[:2], [450,450]).astype(int)),\n",
    "#                       tuple(np.multiply(sample_coords[2:], [450,450]).astype(int)), \n",
    "#                             (255,0,0), 2)\n",
    "#         # Controls the label rectangle\n",
    "#         cv2.rectangle(frame, \n",
    "#                       tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int), \n",
    "#                                     [0,-30])),\n",
    "#                       tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "#                                     [80,0])), \n",
    "#                             (255,0,0), -1)\n",
    "        \n",
    "#         # Controls the text rendered\n",
    "#         cv2.putText(frame, 'face', tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "#                                                [0,-5])),\n",
    "#                     cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    \n",
    "#     cv2.imshow('EyeTrack', frame)\n",
    "    \n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520ffbec-0dce-4a66-a05b-6473ee985a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f662bd8a-1efa-4530-b60a-2f38b39eecde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
